{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Reccurrent-Neural-Network.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvTNMdbsB4vK"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_fpath = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt', \n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = codecs.open(data_fpath, 'r', encoding='utf8').read()\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "t9eRhsS4CSmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание словаря символов\n",
        "vocab = sorted(set(text)) # отбор и соритровка уникалных слов в тексте\n",
        "VOCAB_SIZE = len(vocab) # размер словаря\n",
        "\n",
        "print('Vocab: {}'.format(vocab))\n",
        "print('{} unique characters'.format(VOCAB_SIZE))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)} # создание словаря индекс:символ\n",
        "idx2char = np.array(vocab) # массив словаря\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]) # посимвольное превращение текста в последовательность целых чисел\n",
        "\n",
        "print('Example of the encoded text: {}'.format(text_as_int[:13]))"
      ],
      "metadata": {
        "id": "z0YjIRlHCUTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 1000 # длина последовательности\n",
        "BATCH_SIZE = 64 # размер батча\n",
        "\n",
        "input_seqs = [] # входящая последовательность\n",
        "target_seqs = [] # целевая последовательность\n",
        "\n",
        "num_seqs = len(text_as_int) // (SEQ_LEN+1)\n",
        "\n",
        "for i in range(num_seqs):\n",
        "    seq = text_as_int[i:i+SEQ_LEN+1]\n",
        "    input_seqs.append(np.array(seq[:-1])) # входящая последовательность без последнего символа <START\n",
        "    target_seqs.append(np.array(seq[1:])) # целевая последовательность без первого символа START>\n",
        "\n",
        "input_seqs = np.array(input_seqs)\n",
        "target_seqs = np.array(target_seqs)\n",
        "\n",
        "input_seqs = input_seqs[:(len(input_seqs)//BATCH_SIZE)*BATCH_SIZE]\n",
        "target_seqs = target_seqs[:(len(input_seqs)//BATCH_SIZE)*BATCH_SIZE]\n",
        "\n",
        "print('Input: {} ...'.format([idx2char[i] for i in input_seqs[0][:15]]))\n",
        "print('Target: {} ...'.format([idx2char[i] for i in target_seqs[0][:15]]))"
      ],
      "metadata": {
        "id": "BSsDnwD_CWB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для создания модели генерации текста\n",
        "\n",
        "def build_model(batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=(batch_size, None)), # превращение входящих батчей последовательностей в эмбеддинги\n",
        "        tf.keras.layers.GRU(256, return_sequences=True, stateful=True), # реккуррентный слой возвращабющий всю последовательность и состоряния для каждого элемента\n",
        "        tf.keras.layers.Dense(VOCAB_SIZE), # слой генерации текста, возвращает логиты\n",
        "    ])\n",
        "    model.build()\n",
        "    return model\n",
        "\n",
        "model = build_model(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "oIp5TRtcCch0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# компиляция и обучение модели\n",
        "EPOCHS = 100\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(input_seqs, \n",
        "                    target_seqs, \n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "b5tCAhVDCiWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# модель для инференса с входом батча 1\n",
        "model_inf = build_model(1)\n",
        "\n",
        "# копирование весов из каждого слоя обученной модели в каждый слой модели инференса\n",
        "for i in range(len(model_inf.layers)):\n",
        "    for j in range(len(model_inf.layers[i].weights)):\n",
        "        model_inf.layers[i].weights[j].assign(model.layers[i].weights[j])"
      ],
      "metadata": {
        "id": "kUWfyaqqCkbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для генерациии текста с начальным зерном, конечной длинной текста и температурой\n",
        "def generate_text(model, seed, out_len, temperature:float):\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Обнуляем состояние модели\n",
        "    model.reset_states()\n",
        "    \n",
        "    # Конвертируем входную цепочку в индексы\n",
        "    inp = np.array([char2idx[s] for s in seed])\n",
        "\n",
        "    for i in range(out_len):\n",
        "\n",
        "        # Получаем предсказания для входной цепочки inp\n",
        "        # pred - матрица размерности (длина цепочки, распределение по классам)\n",
        "        # На первой итерации цикла длина цепочки равна длине seed, а затем длина равна 1\n",
        "        pred = model(inp[None, ...])[0]\n",
        "\n",
        "        # Для получения символа сэмплируем из распределения\n",
        "        # БОльшая температура соответствует более случайному предсказанию символа\n",
        "        pred = pred / temperature\n",
        "        pred_c = tf.random.categorical(pred, num_samples=1)[-1][0].numpy()\n",
        "        \n",
        "        text_generated.append(idx2char[pred_c])\n",
        "        \n",
        "        # Новый вход -- только что сгенерированный символ\n",
        "        inp = np.array([pred_c])\n",
        "\n",
        "    return (seed + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "0igjYG3XCmxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500, temperature=1))"
      ],
      "metadata": {
        "id": "o9sm2y3kCoT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500, temperature=0.5))"
      ],
      "metadata": {
        "id": "bICdvY04Cqqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500, temperature=1.5))"
      ],
      "metadata": {
        "id": "3B9UzMfFCsXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=(batch_size, None)),\n",
        "        tf.keras.layers.GRU(256, return_sequences=True, stateful=True, dropout=.5),\n",
        "        tf.keras.layers.GRU(256, return_sequences=True, stateful=True, dropout=.5),\n",
        "        tf.keras.layers.Dense(VOCAB_SIZE),\n",
        "    ])\n",
        "    model.build()\n",
        "    return model\n",
        "\n",
        "model = build_model(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "89jZ5k4mCvcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(input_seqs, \n",
        "    target_seqs,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "XbpOirRWCwyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inf = build_model(1)\n",
        "\n",
        "for i in range(len(model_inf.layers)):\n",
        "    for j in range(len(model_inf.layers[i].weights)):\n",
        "        model_inf.layers[i].weights[j].assign(model.layers[i].weights[j])"
      ],
      "metadata": {
        "id": "e0rHZPvnCzBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500, temperature=1))"
      ],
      "metadata": {
        "id": "w1ZzlenfC0PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_IMDB = 20000 # Количество слов в словаре\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data(num_words=VOCAB_SIZE_IMDB)"
      ],
      "metadata": {
        "id": "Iw5jTn8CC2JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.row_stack([train_data, test_data])[0]"
      ],
      "metadata": {
        "id": "mxrTQIKLC3SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# словарь всех слов в корпусе типа ключ:значение\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# добавление спец.токенов в словарь\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "\n",
        "# перевернутый словарь значение:ключ\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "# последовательность индексов в текст\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, ' ') for i in text])\n",
        "\n",
        "# текст в последовательность индексов\n",
        "def encode_review(text):\n",
        "    words = text.lower().split()\n",
        "    words = ['<START>'] + words\n",
        "    idxs = [word_index.get(word, word_index['<UNKNOWN>']) for word in words]\n",
        "    return idxs\n",
        "\n",
        "print('Example of a decoded review: \\n{}'.format(decode_review(train_data[0])))"
      ],
      "metadata": {
        "id": "BzIHAYWjC42z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример декодирования текста\n",
        "d = []\n",
        "for i in range(len(data)):\n",
        "  d.append(decode_review(data[i]))\n",
        "texts = ' '.join(d)"
      ],
      "metadata": {
        "id": "U7V6IAIuMUb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание словаря с помощью Tokenizer\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "target_regex = '!\"#$%&()*+,-./:;=?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
        "# Tokenizer позволяет векторизовать наш корпус, превращая каждое предложение\n",
        "# в последовательность целых чисел, где каждое целое число является индексом\n",
        "# токена во внутреннем словаре\n",
        "tokenizer = Tokenizer(filters=target_regex) # создание объекта Tokenizer с фильтром\n",
        "tokenizer.fit_on_texts(df_m_1 + df_f_1) # анализ корпусов и составление словаря\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1 # размер полученного словаря\n",
        "\n",
        "tokenizer.word_index.get('<PAD>') # словарь типа значение:ключ .get() выдает ключ по слову\n",
        "tokenizer.index_word.get(0) # словарь типа ключ:значение .get() выдает слово по ключу\n",
        "\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE))"
      ],
      "metadata": {
        "id": "yseRY0ysFlXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 256 # Финальная длина последовательности\n",
        "\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_data,\n",
        "    value=word_index[\"<PAD>\"],\n",
        "    padding='post',\n",
        "    maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_data,\n",
        "    value=word_index[\"<PAD>\"],\n",
        "    padding='post',\n",
        "    maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "print(\"Length examples: {}\".format([len(train_data[0]), len(train_data[1])]))\n",
        "print('=====================================')\n",
        "print(\"Entry example: {}\".format(train_data[0]))"
      ],
      "metadata": {
        "id": "0haQDgslMxeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "metadata": {
        "id": "iyZ_8YhFM9kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_SIZE = 16 # Размер векторного представления (эмбеддинга)\n",
        "    \n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Pynq5BVzM3rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "6K1e1KnNM5pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "h7VHu52mM8Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print('Test loss: {:.4f}'.format(results[0]))\n",
        "print('Test accuracy: {:.2f} %'.format(results[1]*100))"
      ],
      "metadata": {
        "id": "Q5lO-T8pM_M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, history.history['acc'], 'bo', label='Training acc')\n",
        "plt.plot(epochs, history.history['val_acc'], 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "UCxz9m6-NDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'It was a really good movie it was so fantastic'\n",
        "\n",
        "encoded = encode_review(text)\n",
        "prediction = model.predict(np.array(encoded)[None, :])[0,0]\n",
        "\n",
        "print(prediction)\n",
        "print('Positive' if prediction > 0.5 else 'Negative')"
      ],
      "metadata": {
        "id": "7OwOjDxKNJW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder"
      ],
      "metadata": {
        "id": "P9A1VxE_NRIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# русско-французкий разговорник\n",
        "data_fpath = '/content/rus.txt'\n",
        "max_sentences = 10000\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "lines = codecs.open(data_fpath, 'r', encoding='utf8').readlines()[:max_sentences]\n",
        "for line in lines:\n",
        "    input_text, target_text, = line.split('\\t')[:2] # разделение на русские и французкие слова\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)"
      ],
      "metadata": {
        "id": "R0DlGBaoNWso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание словаря символов\n",
        "def prepare_vocab(texts):\n",
        "    vocab = sorted(set(''.join(texts)))\n",
        "    # добавление токена начала и конца последовательности\n",
        "    vocab.append('<START>') \n",
        "    vocab.append('<END>')\n",
        "    vocab_size = len(vocab)\n",
        "    char2idx = {u:i for i, u in enumerate(vocab)} # словарь ключ:значение\n",
        "    idx2char = np.array(vocab) # словарь значение:ключ\n",
        "    return vocab_size, char2idx, idx2char \n",
        "\n",
        "INPUT_VOCAB_SIZE, input_char2idx, input_idx2char = prepare_vocab(input_texts)\n",
        "TARGET_VOCAB_SIZE, target_char2idx, target_idx2char = prepare_vocab(target_texts)"
      ],
      "metadata": {
        "id": "oUiEy_4eNYeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# текст в последовательность чисел\n",
        "input_texts_as_int = [[input_char2idx[c] for c in text] for text in input_texts]\n",
        "target_texts_as_int = [[target_char2idx[c] for c in text] for text in target_texts]\n",
        "\n",
        "encoder_input_seqs = [np.array(text) for text in input_texts_as_int] # последовательность для энкодера\n",
        "decoder_input_seqs = [] # входящая последовательность для декодера\n",
        "decoder_target_seqs = [] # целевая последовательность для декодера\n",
        "\n",
        "# добавление спец.токенов начала и конца во входящую и целевую последовательность для декодера\n",
        "for target_text in target_texts_as_int:\n",
        "    decoder_input_seqs.append(np.array([target_char2idx['<START>']] + target_text))\n",
        "    decoder_target_seqs.append(np.array(target_text + [target_char2idx['<END>']]))"
      ],
      "metadata": {
        "id": "fv1A_x4sNbG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# приведение длины всех последовательностей к максимальной\n",
        "max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\n",
        "max_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n",
        "\n",
        "encoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    encoder_input_seqs,\n",
        "    value=input_char2idx[' '],\n",
        "    padding='post',\n",
        "    maxlen=max_enc_seq_length)\n",
        "\n",
        "decoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_input_seqs,\n",
        "    value=target_char2idx[' '],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length)\n",
        "\n",
        "decoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_target_seqs,\n",
        "    value=target_char2idx[' '],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length)"
      ],
      "metadata": {
        "id": "lpUngnfENc7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder с одним LSTM слоем\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embed(x)\n",
        "        _, h, c = self.lstm(out)\n",
        "        state = (h, c)\n",
        "        return state\n",
        "\n",
        "# Decoder с одним LSTM слоем\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE, activation='softmax')\n",
        "        \n",
        "    def call(self, x, init_state):\n",
        "        out = self.embed(x)\n",
        "        out, h, c = self.lstm(out, initial_state=init_state)\n",
        "        out = self.fc(out)\n",
        "        state = (h, c)\n",
        "        return out, state"
      ],
      "metadata": {
        "id": "VTqZYXnlNefP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H_SIZE = 256 # Размерность скрытого состояния LSTM\n",
        "EMB_SIZE = 256 # размерность эмбеддингов (и для входных и для выходных цепочек)\n",
        "\n",
        "encoder_model = Encoder()\n",
        "decoder_model = Decoder()\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "klxKdYFBNgB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64 # размер входящего батча\n",
        "EPOCHS = 50\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "seq2seq.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "seq2seq.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "Nlh9E8wuNhXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# инференс для последовательности. При вызове возвращает перевод на другом языке\n",
        "def seq2seq_inference(input_seq):\n",
        "    state = encoder_model(input_seq) # получение скрытого состояия и памяти от энкодера\n",
        "\n",
        "    target_seq = np.array([[target_char2idx['<START>']]]) # целевая последовательность из спец.токена для декодера\n",
        "\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state) # передача скрытого состояния и памяти в декодер\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :]) # \n",
        "        sampled_char = target_idx2char[sampled_token_index] # из числа в слово\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # прекращение генерации при получении спец токена либо достижения максимальной последовательности\n",
        "        if (sampled_char == '<END>' or\n",
        "           len(decoded_sentence) > max_dec_seq_length):\n",
        "            break\n",
        "\n",
        "        target_seq = np.array([[sampled_token_index]]) #\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "haxP79HhNizN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(0, 20):\n",
        "    input_seq = encoder_input_seqs[seq_index: seq_index + 1]\n",
        "    decoded_sentence = seq2seq_inference(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Result sentence:', decoded_sentence)\n",
        "    print('Target sentence:', target_texts[seq_index])"
      ],
      "metadata": {
        "id": "ayMlSxyrNkDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder с двумя LSTM слоями\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True)\n",
        "        self.lstm_2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embed(x)\n",
        "        _, h_1, c_1 = self.lstm_1(out)\n",
        "        _, h_2, c_2 = self.lstm_2(out)\n",
        "        state = ((h_1, c_1), (h_2, c_2))\n",
        "        return state\n",
        "\n",
        "# Decoder с двумя LSTM слоями\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, return_state=True)\n",
        "        self.lstm_2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE, activation='softmax')\n",
        "        \n",
        "    def call(self, x, init_state):\n",
        "        out = self.embed(x)\n",
        "        init_state_1, init_state_2 = init_state\n",
        "        out_1, h_1, c_1 = self.lstm_1(out, initial_state=init_state_1)\n",
        "        out_2, h_2, c_2 = self.lstm_2(out_1, initial_state=init_state_2)\n",
        "        out_fc = self.fc(out_2)\n",
        "        state = ((h_1, c_1), (h_2, c_2))\n",
        "        return out_fc, state\n",
        "\n",
        "encoder_model = Encoder()\n",
        "decoder_model = Decoder()\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "mzR6n30VNouJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "seq2seq.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
        "seq2seq.fit([encoder_input_seqs, decoder_input_seqs],\n",
        "            decoder_target_seqs,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "ns4YaHg2Nqu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(0, 20):\n",
        "    input_seq = encoder_input_seqs[seq_index: seq_index + 1]\n",
        "    decoded_sentence = seq2seq_inference(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Result sentence:', decoded_sentence)\n",
        "    print('Target sentence:', target_texts[seq_index])"
      ],
      "metadata": {
        "id": "XwEeXjjBNsTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder с двунаправленым LSMT слоем\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm_bi = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True))\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embed(x)\n",
        "        _, h1, c1, h2, c2 = self.lstm_bi(out)\n",
        "        state = ((h1, c1), (h2, c2))\n",
        "        return state\n",
        "\n",
        "encoder_model = Encoder()\n",
        "decoder_model = Decoder()\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "clU2jPZbNu7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "seq2seq.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
        "seq2seq.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "qt6FtxYiNwWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(0, 20):\n",
        "    input_seq = encoder_input_seqs[seq_index: seq_index + 1]\n",
        "    decoded_sentence = seq2seq_inference(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Result sentence:', decoded_sentence)\n",
        "    print('Target sentence:', target_texts[seq_index])"
      ],
      "metadata": {
        "id": "gQWdeFxANxzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}